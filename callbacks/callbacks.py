"""Functions to call during the training loop."""

import numpy as np
import torch
from sklearn.metrics import classification_report, roc_auc_score

def class_report(model, criterion, dataloader, batch_size):
    """Classification report generated by the model when predicting on the
    dataset supplied by dataloader. Prints this classification report to console, returns the
    loss calculated by criterion.
    """
    size_dataset = len(dataloader) * batch_size
    predictions = np.zeros(size_dataset)
    true_labels = np.zeros(size_dataset)
    indx = 0

    with torch.no_grad():
        model.eval()
        for batch in dataloader:
            image_batch, label_batch = batch
            outputs = model(image_batch)

            loss = criterion(outputs, label_batch)
            loss = loss.item()
            # get the index of the max value
            _, predicted_batch = torch.max(outputs, 1)

            predictions[indx*batch_size:(indx+1)*batch_size] = predicted_batch.to('cpu').numpy()
            true_labels[indx*batch_size:(indx+1)*batch_size] = label_batch.to('cpu').numpy()
            indx += 1

            if len(batch[0]) != batch_size:
                predictions = predictions[:(indx-1)*batch_size + len(batch[0])]
                true_labels = true_labels[:(indx-1)*batch_size + len(batch[0])]
                break

    true_labels = true_labels.astype(int)
    predictions = predictions.astype(int)
    
    # one hot encode for roc_auc_score
    one_hot_preds = np.zeros((size_dataset, 3))
    one_hot_labels = np.zeros((size_dataset, 3))
    for i, (p, l) in enumerate(zip(predictions, true_labels)):
        one_hot_preds[i, p] = 1
        one_hot_labels[i, l] = 1

    print(classification_report(true_labels, predictions))
    #print(f"Area under curve :\t{auc}")
    print("\nloss:\t\t%.3f" % (loss))

    return loss#, auc

def print_statistics(outputs, labels):
    """Calculates the accuracy of output predictions to the given labels"""
    class_correct = np.zeros(3)
    class_total = np.zeros(3)
    _, predicted = torch.max(outputs, 1)
    binary_correct = (predicted == labels)

    for j, correct in enumerate(binary_correct):
        label = labels[j]
        class_correct[int(label)] += correct
        class_total[int(label)] += 1

    overall_accuracy = 100 * np.sum(class_correct) / np.sum(class_total)
    average_accuracy = 100./3 * np.sum((class_correct/class_total))
    return overall_accuracy, average_accuracy

#def save_model():

#def verbose_print():
