{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitminiconda3virtualenv7f55c4b6fee2498189fa81f61722d656",
   "display_name": "Python 3.7.6 64-bit ('miniconda3': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from torchsummary import summary\n",
    "import PIL\n",
    "\n",
    "# Ignore warnings\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_data(root_dir, augmentations=None, batch_size=1, shuffle=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        root_dir (string): path to the class folders\n",
    "        augmentations (callable, optional): transformations to randomly apply to images, default is None\n",
    "        batch_size (int): number of images in each batch, default is 1\n",
    "        shuffle (bool): to shuffle the dataset or not, default is false\n",
    "    Returns:\n",
    "        A batched labeled image dataloader from the root_dir with applied augmentations.\n",
    "    \"\"\"\n",
    "    if augmentations is None:\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.Resize(size=(224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0, 0, 0), ((255, 255, 255))),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), ((0.229, 0.224, 0.225)))\n",
    "\n",
    "        ])\n",
    "    else:\n",
    "        data_transform = augmentations\n",
    "\n",
    "    dataset = datasets.ImageFolder(root=root_dir,\n",
    "                                    transform=data_transform)\n",
    "    data_loader = DataLoader(dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=4)\n",
    "    return data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(48, 64), scale=(0.8, 1.0), ratio=(1, 1)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    # rescale to be in (0, 1)\n",
    "    transforms.Normalize((0, 0, 0), ((255, 255, 255)))\n",
    "])\n",
    "train = image_data(\"IMGS/IR/train\", batch_size=32, shuffle=True)\n",
    "valid = image_data(\"IMGS/IR/valid\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_img_batch(data_loader):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    #data = iter(data_loader)\n",
    "    for batch in data_loader:\n",
    "        img_batch, label_batch = batch\n",
    "        for i, img in enumerate(img_batch):\n",
    "            # channels last\n",
    "            img = img.permute(1, 2, 0)\n",
    "            label = label_batch[i]\n",
    "            plt.subplot(3, 3, i+1)\n",
    "            plt.imshow(img.numpy())\n",
    "            plt.title(label.numpy())\n",
    "            plt.axis('off')\n",
    "            if i == 8:\n",
    "                plt.show()\n",
    "                break\n",
    "        break\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"573.118125pt\" version=\"1.1\" viewBox=\"0 0 568.164706 573.118125\" width=\"568.164706pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 573.118125 \nL 568.164706 573.118125 \nL 568.164706 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g clip-path=\"url(#p60cb7a32c5)\">\n    <image height=\"160\" id=\"image497c0bce97\" transform=\"scale(1 -1)translate(0 -160)\" width=\"160\" x=\"7.2\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAYAAACLz2ctAAAABHNCSVQICAgIfAhkiAAAAa1JREFUeJzt0jEBwDAAw7Cs/Dl3MHxUQuDD37Y7iJw6gLcZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSf0mKQI/DlM90AAAAABJRU5ErkJggg==\" y=\"-22.200478\"/>\n   </g>\n   <g id=\"text_1\">\n    <!-- 0 -->\n    <defs>\n     <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n    </defs>\n    <g transform=\"translate(83.323676 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g clip-path=\"url(#p8148b3355a)\">\n    <image height=\"160\" id=\"imagecac692e186\" transform=\"scale(1 -1)translate(0 -160)\" width=\"160\" x=\"204.141176\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAYAAACLz2ctAAAABHNCSVQICAgIfAhkiAAAAa1JREFUeJzt0jEBwDAAw7Cs/Dl3MHxUQuDD37Y7iJw6gLcZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSf0mKQI/DlM90AAAAABJRU5ErkJggg==\" y=\"-22.200478\"/>\n   </g>\n   <g id=\"text_2\">\n    <!-- 0 -->\n    <g transform=\"translate(280.264853 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g clip-path=\"url(#p4da0d7d3bf)\">\n    <image height=\"160\" id=\"imagee9f5cbdb44\" transform=\"scale(1 -1)translate(0 -160)\" width=\"160\" x=\"401.082353\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAYAAACLz2ctAAAABHNCSVQICAgIfAhkiAAAAa1JREFUeJzt0jEBwDAAw7Cs/Dl3MHxUQuDD37Y7iJw6gLcZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSf0mKQI/DlM90AAAAABJRU5ErkJggg==\" y=\"-22.200478\"/>\n   </g>\n   <g id=\"text_3\">\n    <!-- 0 -->\n    <g transform=\"translate(477.206029 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g clip-path=\"url(#p0c0ec5b88d)\">\n    <image height=\"160\" id=\"image8f9dcc4d98\" transform=\"scale(1 -1)translate(0 -160)\" width=\"160\" x=\"7.2\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAYAAACLz2ctAAAABHNCSVQICAgIfAhkiAAAAa1JREFUeJzt0jEBwDAAw7Cs/Dl3MHxUQuDD37Y7iJw6gLcZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSf0mKQI/DlM90AAAAABJRU5ErkJggg==\" y=\"-214.059301\"/>\n   </g>\n   <g id=\"text_4\">\n    <!-- 0 -->\n    <g transform=\"translate(83.323676 208.176949)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_5\">\n   <g clip-path=\"url(#pc5f0daa793)\">\n    <image height=\"160\" id=\"image429172b9c9\" transform=\"scale(1 -1)translate(0 -160)\" width=\"160\" x=\"204.141176\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAYAAACLz2ctAAAABHNCSVQICAgIfAhkiAAAAa1JREFUeJzt0jEBwDAAw7Cs/Dl3MHxUQuDD37Y7iJw6gLcZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSf0mKQI/DlM90AAAAABJRU5ErkJggg==\" y=\"-214.059301\"/>\n   </g>\n   <g id=\"text_5\">\n    <!-- 0 -->\n    <g transform=\"translate(280.264853 208.176949)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_6\">\n   <g clip-path=\"url(#p2b865eed41)\">\n    <image height=\"160\" id=\"image64f831d4ea\" transform=\"scale(1 -1)translate(0 -160)\" width=\"160\" x=\"401.082353\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAYAAACLz2ctAAAABHNCSVQICAgIfAhkiAAAAa1JREFUeJzt0jEBwDAAw7Cs/Dl3MHxUQuDD37Y7iJw6gLcZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSf0mKQI/DlM90AAAAABJRU5ErkJggg==\" y=\"-214.059301\"/>\n   </g>\n   <g id=\"text_6\">\n    <!-- 2 -->\n    <defs>\n     <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n    </defs>\n    <g transform=\"translate(477.206029 208.176949)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-50\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_7\">\n   <g clip-path=\"url(#pb12e7b967e)\">\n    <image height=\"160\" id=\"imageab8905c98a\" transform=\"scale(1 -1)translate(0 -160)\" width=\"160\" x=\"7.2\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAYAAACLz2ctAAAABHNCSVQICAgIfAhkiAAAAa1JREFUeJzt0jEBwDAAw7Cs/Dl3MHxUQuDD37Y7iJw6gLcZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSf0mKQI/DlM90AAAAABJRU5ErkJggg==\" y=\"-405.918125\"/>\n   </g>\n   <g id=\"text_7\">\n    <!-- 0 -->\n    <g transform=\"translate(83.323676 400.035772)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_8\">\n   <g clip-path=\"url(#pe759572c25)\">\n    <image height=\"160\" id=\"imageee5f7b42eb\" transform=\"scale(1 -1)translate(0 -160)\" width=\"160\" x=\"204.141176\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAYAAACLz2ctAAAABHNCSVQICAgIfAhkiAAAAa1JREFUeJzt0jEBwDAAw7Cs/Dl3MHxUQuDD37Y7iJw6gLcZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSf0mKQI/DlM90AAAAABJRU5ErkJggg==\" y=\"-405.918125\"/>\n   </g>\n   <g id=\"text_8\">\n    <!-- 0 -->\n    <g transform=\"translate(280.264853 400.035772)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_9\">\n   <g clip-path=\"url(#p6bffab75b1)\">\n    <image height=\"160\" id=\"image2ce05389b1\" transform=\"scale(1 -1)translate(0 -160)\" width=\"160\" x=\"401.082353\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAYAAACLz2ctAAAABHNCSVQICAgIfAhkiAAAAa1JREFUeJzt0jEBwDAAw7Cs/Dl3MHxUQuDD37Y7iJw6gLcZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSRmQlAFJGZCUAUkZkJQBSf0mKQI/DlM90AAAAABJRU5ErkJggg==\" y=\"-405.918125\"/>\n   </g>\n   <g id=\"text_9\">\n    <!-- 0 -->\n    <g transform=\"translate(477.206029 400.035772)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p60cb7a32c5\">\n   <rect height=\"159.882353\" width=\"159.882353\" x=\"7.2\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p8148b3355a\">\n   <rect height=\"159.882353\" width=\"159.882353\" x=\"204.141176\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p4da0d7d3bf\">\n   <rect height=\"159.882353\" width=\"159.882353\" x=\"401.082353\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p0c0ec5b88d\">\n   <rect height=\"159.882353\" width=\"159.882353\" x=\"7.2\" y=\"214.176949\"/>\n  </clipPath>\n  <clipPath id=\"pc5f0daa793\">\n   <rect height=\"159.882353\" width=\"159.882353\" x=\"204.141176\" y=\"214.176949\"/>\n  </clipPath>\n  <clipPath id=\"p2b865eed41\">\n   <rect height=\"159.882353\" width=\"159.882353\" x=\"401.082353\" y=\"214.176949\"/>\n  </clipPath>\n  <clipPath id=\"pb12e7b967e\">\n   <rect height=\"159.882353\" width=\"159.882353\" x=\"7.2\" y=\"406.035772\"/>\n  </clipPath>\n  <clipPath id=\"pe759572c25\">\n   <rect height=\"159.882353\" width=\"159.882353\" x=\"204.141176\" y=\"406.035772\"/>\n  </clipPath>\n  <clipPath id=\"p6bffab75b1\">\n   <rect height=\"159.882353\" width=\"159.882353\" x=\"401.082353\" y=\"406.035772\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": "<Figure size 720x720 with 9 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_img_batch(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = models.mobilenet_v2(pretrained=True).train(mode=False)\n",
    "        self.fc1 = nn.Linear(1000, 3)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 32, 112, 112]             864\n       BatchNorm2d-2         [-1, 32, 112, 112]              64\n             ReLU6-3         [-1, 32, 112, 112]               0\n            Conv2d-4         [-1, 32, 112, 112]             288\n       BatchNorm2d-5         [-1, 32, 112, 112]              64\n             ReLU6-6         [-1, 32, 112, 112]               0\n            Conv2d-7         [-1, 16, 112, 112]             512\n       BatchNorm2d-8         [-1, 16, 112, 112]              32\n  InvertedResidual-9         [-1, 16, 112, 112]               0\n           Conv2d-10         [-1, 96, 112, 112]           1,536\n      BatchNorm2d-11         [-1, 96, 112, 112]             192\n            ReLU6-12         [-1, 96, 112, 112]               0\n           Conv2d-13           [-1, 96, 56, 56]             864\n      BatchNorm2d-14           [-1, 96, 56, 56]             192\n            ReLU6-15           [-1, 96, 56, 56]               0\n           Conv2d-16           [-1, 24, 56, 56]           2,304\n      BatchNorm2d-17           [-1, 24, 56, 56]              48\n InvertedResidual-18           [-1, 24, 56, 56]               0\n           Conv2d-19          [-1, 144, 56, 56]           3,456\n      BatchNorm2d-20          [-1, 144, 56, 56]             288\n            ReLU6-21          [-1, 144, 56, 56]               0\n           Conv2d-22          [-1, 144, 56, 56]           1,296\n      BatchNorm2d-23          [-1, 144, 56, 56]             288\n            ReLU6-24          [-1, 144, 56, 56]               0\n           Conv2d-25           [-1, 24, 56, 56]           3,456\n      BatchNorm2d-26           [-1, 24, 56, 56]              48\n InvertedResidual-27           [-1, 24, 56, 56]               0\n           Conv2d-28          [-1, 144, 56, 56]           3,456\n      BatchNorm2d-29          [-1, 144, 56, 56]             288\n            ReLU6-30          [-1, 144, 56, 56]               0\n           Conv2d-31          [-1, 144, 28, 28]           1,296\n      BatchNorm2d-32          [-1, 144, 28, 28]             288\n            ReLU6-33          [-1, 144, 28, 28]               0\n           Conv2d-34           [-1, 32, 28, 28]           4,608\n      BatchNorm2d-35           [-1, 32, 28, 28]              64\n InvertedResidual-36           [-1, 32, 28, 28]               0\n           Conv2d-37          [-1, 192, 28, 28]           6,144\n      BatchNorm2d-38          [-1, 192, 28, 28]             384\n            ReLU6-39          [-1, 192, 28, 28]               0\n           Conv2d-40          [-1, 192, 28, 28]           1,728\n      BatchNorm2d-41          [-1, 192, 28, 28]             384\n            ReLU6-42          [-1, 192, 28, 28]               0\n           Conv2d-43           [-1, 32, 28, 28]           6,144\n      BatchNorm2d-44           [-1, 32, 28, 28]              64\n InvertedResidual-45           [-1, 32, 28, 28]               0\n           Conv2d-46          [-1, 192, 28, 28]           6,144\n      BatchNorm2d-47          [-1, 192, 28, 28]             384\n            ReLU6-48          [-1, 192, 28, 28]               0\n           Conv2d-49          [-1, 192, 28, 28]           1,728\n      BatchNorm2d-50          [-1, 192, 28, 28]             384\n            ReLU6-51          [-1, 192, 28, 28]               0\n           Conv2d-52           [-1, 32, 28, 28]           6,144\n      BatchNorm2d-53           [-1, 32, 28, 28]              64\n InvertedResidual-54           [-1, 32, 28, 28]               0\n           Conv2d-55          [-1, 192, 28, 28]           6,144\n      BatchNorm2d-56          [-1, 192, 28, 28]             384\n            ReLU6-57          [-1, 192, 28, 28]               0\n           Conv2d-58          [-1, 192, 14, 14]           1,728\n      BatchNorm2d-59          [-1, 192, 14, 14]             384\n            ReLU6-60          [-1, 192, 14, 14]               0\n           Conv2d-61           [-1, 64, 14, 14]          12,288\n      BatchNorm2d-62           [-1, 64, 14, 14]             128\n InvertedResidual-63           [-1, 64, 14, 14]               0\n           Conv2d-64          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-65          [-1, 384, 14, 14]             768\n            ReLU6-66          [-1, 384, 14, 14]               0\n           Conv2d-67          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-68          [-1, 384, 14, 14]             768\n            ReLU6-69          [-1, 384, 14, 14]               0\n           Conv2d-70           [-1, 64, 14, 14]          24,576\n      BatchNorm2d-71           [-1, 64, 14, 14]             128\n InvertedResidual-72           [-1, 64, 14, 14]               0\n           Conv2d-73          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-74          [-1, 384, 14, 14]             768\n            ReLU6-75          [-1, 384, 14, 14]               0\n           Conv2d-76          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-77          [-1, 384, 14, 14]             768\n            ReLU6-78          [-1, 384, 14, 14]               0\n           Conv2d-79           [-1, 64, 14, 14]          24,576\n      BatchNorm2d-80           [-1, 64, 14, 14]             128\n InvertedResidual-81           [-1, 64, 14, 14]               0\n           Conv2d-82          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-83          [-1, 384, 14, 14]             768\n            ReLU6-84          [-1, 384, 14, 14]               0\n           Conv2d-85          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-86          [-1, 384, 14, 14]             768\n            ReLU6-87          [-1, 384, 14, 14]               0\n           Conv2d-88           [-1, 64, 14, 14]          24,576\n      BatchNorm2d-89           [-1, 64, 14, 14]             128\n InvertedResidual-90           [-1, 64, 14, 14]               0\n           Conv2d-91          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-92          [-1, 384, 14, 14]             768\n            ReLU6-93          [-1, 384, 14, 14]               0\n           Conv2d-94          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-95          [-1, 384, 14, 14]             768\n            ReLU6-96          [-1, 384, 14, 14]               0\n           Conv2d-97           [-1, 96, 14, 14]          36,864\n      BatchNorm2d-98           [-1, 96, 14, 14]             192\n InvertedResidual-99           [-1, 96, 14, 14]               0\n          Conv2d-100          [-1, 576, 14, 14]          55,296\n     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n           ReLU6-102          [-1, 576, 14, 14]               0\n          Conv2d-103          [-1, 576, 14, 14]           5,184\n     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n           ReLU6-105          [-1, 576, 14, 14]               0\n          Conv2d-106           [-1, 96, 14, 14]          55,296\n     BatchNorm2d-107           [-1, 96, 14, 14]             192\nInvertedResidual-108           [-1, 96, 14, 14]               0\n          Conv2d-109          [-1, 576, 14, 14]          55,296\n     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n           ReLU6-111          [-1, 576, 14, 14]               0\n          Conv2d-112          [-1, 576, 14, 14]           5,184\n     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n           ReLU6-114          [-1, 576, 14, 14]               0\n          Conv2d-115           [-1, 96, 14, 14]          55,296\n     BatchNorm2d-116           [-1, 96, 14, 14]             192\nInvertedResidual-117           [-1, 96, 14, 14]               0\n          Conv2d-118          [-1, 576, 14, 14]          55,296\n     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n           ReLU6-120          [-1, 576, 14, 14]               0\n          Conv2d-121            [-1, 576, 7, 7]           5,184\n     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n           ReLU6-123            [-1, 576, 7, 7]               0\n          Conv2d-124            [-1, 160, 7, 7]          92,160\n     BatchNorm2d-125            [-1, 160, 7, 7]             320\nInvertedResidual-126            [-1, 160, 7, 7]               0\n          Conv2d-127            [-1, 960, 7, 7]         153,600\n     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n           ReLU6-129            [-1, 960, 7, 7]               0\n          Conv2d-130            [-1, 960, 7, 7]           8,640\n     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n           ReLU6-132            [-1, 960, 7, 7]               0\n          Conv2d-133            [-1, 160, 7, 7]         153,600\n     BatchNorm2d-134            [-1, 160, 7, 7]             320\nInvertedResidual-135            [-1, 160, 7, 7]               0\n          Conv2d-136            [-1, 960, 7, 7]         153,600\n     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n           ReLU6-138            [-1, 960, 7, 7]               0\n          Conv2d-139            [-1, 960, 7, 7]           8,640\n     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n           ReLU6-141            [-1, 960, 7, 7]               0\n          Conv2d-142            [-1, 160, 7, 7]         153,600\n     BatchNorm2d-143            [-1, 160, 7, 7]             320\nInvertedResidual-144            [-1, 160, 7, 7]               0\n          Conv2d-145            [-1, 960, 7, 7]         153,600\n     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n           ReLU6-147            [-1, 960, 7, 7]               0\n          Conv2d-148            [-1, 960, 7, 7]           8,640\n     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n           ReLU6-150            [-1, 960, 7, 7]               0\n          Conv2d-151            [-1, 320, 7, 7]         307,200\n     BatchNorm2d-152            [-1, 320, 7, 7]             640\nInvertedResidual-153            [-1, 320, 7, 7]               0\n          Conv2d-154           [-1, 1280, 7, 7]         409,600\n     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n           ReLU6-156           [-1, 1280, 7, 7]               0\n         Dropout-157                 [-1, 1280]               0\n          Linear-158                 [-1, 1000]       1,281,000\n     MobileNetV2-159                 [-1, 1000]               0\n             Net-160                 [-1, 1000]               0\n================================================================\nTotal params: 3,504,872\nTrainable params: 3,504,872\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 152.88\nParams size (MB): 13.37\nEstimated Total Size (MB): 166.83\n----------------------------------------------------------------\n\n"
    },
    {
     "data": {
      "text/plain": "(tensor(3504872), tensor(3504872))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "summary(net, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Starting training...\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.730\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.517\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.304\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.359\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.348\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.233\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.167\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.035\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.071\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.009\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 5.980\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.011\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.048\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 5.958\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.039\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.050\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.100\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.034\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.035\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.084\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 5.972\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.028\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.067\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.018\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.043\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 5.972\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.096\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 5.999\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.038\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.115\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.011\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.083\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.085\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.086\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.063\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.073\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.067\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.079\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.092\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.043\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 5.929\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.040\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.032\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.079\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.030\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.003\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.004\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 5.983\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.044\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.049\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 6.011\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 5.988\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 5.954\ntorch.Size([32, 1000]) torch.Size([32])\ntorch.Size([32, 1000]) torch.Size([32])\nEpoch 1 :\tTraining loss : 5.967\ntorch.Size([32, 1000]) torch.Size([32])\n"
    }
   ],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward, backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(np.shape(outputs), np.shape(labels))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2 == 1:  # print every 2 mini-batches\n",
    "            print('Epoch %d :\\tTraining loss : %.3f' % \n",
    "                    (epoch + 1, running_loss / 2))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-cd2eddd56846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: eval() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "class_correct = list(0. for i in range(3))\n",
    "class_total = list(0. for i in range(3))\n",
    "with torch.no_grad():\n",
    "    for data in valid:\n",
    "        imgs, labels = data\n",
    "        outputs = net(imgs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(150):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "classes = ('nonbreaking', 'plunge', 'spill')\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Accuracy of %5s : %2d %%\" % (\n",
    "        classes[i], 100*class_correct[i]/class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_report(model, valid, VAL_BATCH_SIZE, STEP_SIZE_VALID):\n",
    "    print(\"\\ngenerating validation report...\")\n",
    "    pred = np.array()\n",
    "    la = np.array()\n",
    "    for data in valid:\n",
    "        imgs, labels = data\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        np.append(pred, c)\n",
    "        np.append(la, labels)\n",
    "    #p = model.predict(valid_data, steps=STEP_SIZE_VALID)\n",
    "    #preds = np.zeros_like(p)\n",
    "    #preds[np.arange(len(p)), p.argmax(1)] = 1\n",
    "    #labels = np.zeros_like(preds)\n",
    "    for i in range(STEP_SIZE_VALID):\n",
    "        _, labelBatch = next(valid_data)\n",
    "        labels[i*VAL_BATCH_SIZE: (i+1)*VAL_BATCH_SIZE] = labelBatch\n",
    "    print(classification_report(labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}